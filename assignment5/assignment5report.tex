\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage[margin=0.8in]{geometry}
\begin{document}
{\bf Names:} Jack Bracewell, Milan Misak, Craig Ellis \\
{\bf Usernames:} jb2910, mm5510, ce710 \\
{\bf Group Number: 28}  \\ \\

\section*{Assignment 5: T-test}

\subsection*{Evaluation results}

\subsubsection*{Experimental results using clean and noisy data}
\begin{table}
\centering
\begin{tabular}{l | r r r}
Emotion   & Trees   & Networks & CBR     \\
\hline
Anger     & 28.4644 & 24.4726  & 24.0602 \\
Disgust   & 33.7349 & 60.0536  & 53.5484 \\
Fear      & 16.2393 & 46.3950  & 42.6036 \\
Happiness & 73.4793 & 77.4775  & 72.4211 \\
Sadness   & 33.5484 & 39.4366  & 34.4371 \\
Surprise  & 53.0612 & 75.4808  & 77.0563 \\
\end{tabular}
\caption{F1 measures}
\end{table}

\subsubsection*{T-test results using clean and noisy data}
\begin{table}
\centering
\begin{tabular}{l l | r r}
Emotion   &          & Clean Data & Noisy Data \\
\hline
Anger     & Tree/Net & 0          & 1          \\
          & Tree/CBR & 1          & 1          \\
          & Net/CBR  & 0          & 0          \\
Disgust   & Tree/Net & 0          & 0          \\
          & Tree/CBR & 0          & 0          \\
          & Net/CBR  & 0          & 0          \\
Fear      & Tree/Net & 1          & 1          \\
          & Tree/CBR & 1          & 0          \\
          & Net/CBR  & 0          & 0          \\
Happiness & Tree/Net & 0          & 1          \\
          & Tree/CBR & 0          & 0          \\
          & Net/CBR  & 0          & 1          \\
Sadness   & Tree/Net & 1          & 1          \\
          & Tree/CBR & 0          & 0          \\
          & Net/CBR  & 0          & 1          \\
Surprise  & Tree/Net & 1          & 0          \\
          & Tree/CBR & 1          & 0          \\
          & Net/CBR  & 0          & 0          \\
\end{tabular}
\caption{F1 measures}
\end{table}

\subsubsection*{T-test results using clean data}

\subsubsection*{T-test results using noisy data}


\subsubsection*{Which algorithm performed better overall in terms of values of $F_1$ measure (part I)? Which algorithm performed better when comparison was performed using the t-test (part II and part III)? Can we claim that this algorithm is a better learning algorithm than the others in general? Why? Why not?}

lorem ipsum.

\subsubsection*{How did you adjust the significance level in order to take into account the fact that you perform a multiple comparison test?}

divide $\alpha$ by k.

\subsubsection*{Which type of t-test did you use and why?}

The special kind.

\subsubsection*{Why do you think t-test was performed on the classification error and not the F1 measure?}

Something about classification error having higher varience maybe.

\subsubsection*{What is the trade-off between the number of folds you use and the number of examples per fold? In other words, what is going to happen if you use more folds, so you will have fewer examples per fold, or if you use fewer folds, so you will have more examples per fold?}

This shan't be too difficult.

\subsubsection*{Suppose that we want to add some new emotions to the existing dataset. Which of the examined algorithms are more suitable for incorporating the new classes in terms of engineering effort? Which algorithms need to undergo radical changes in order to include new classes?}

Surely CBR is the easiest? Neural networks will need new optimal shit so that might be hardest?

\subsection*{Implementation details}
\begin{itemize}
  \item RETRIEVE:
    We Cycle through all the cases in the structure, picking the cases with the 3 highest similarities to the case we are attempting to classify (3-NN). If there are more than 3 with the highest similarity then they are all included in the "voting", and not discarded. For example, case one has a similarity of 79, case two has 78, and cases three and four both have similarities of 77. We cannot decide which of cases three and four to keep in the 'top three', so we include both.
  \item REUSE:
    We simply assign the solution value from the retrieved case to the case we are classifying.
  \item RETAIN:
    A new case is made with the corresponding AU Vector and solution that was found. The weight is a value used for calculating similarity, and is set to 0.8 for every new case, because that is roughly the classification rate, ie - how much we should trust the prediction, compared to the training data which we know is correct.
  \item CBR cases:
    Each case has an AUVector, a solution, and a typicality value, the use of which is explained previously.
\end{itemize}

\end{document}
