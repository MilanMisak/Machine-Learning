There are 2 cases when there needs to be a decision made about classification which cannot be made by the trees themselves. Firstly, it is where none of the trees have returned a positive value, secondly it is when multiple trees positively classify the same example. Initially we chose to randomise the classification, this was our benchmark. The obvious drawbacks of choosing randomly are that it is a complete guess, and should intuitively not be used if there is more information available. Generation of pseudo random numbers is also a rather expensive task if time is a factor.

Then we decided to select the value which had been chosen the most so far without contention. This was slightly worse than random on average, it is highly dependent on the order of the data. This method then became superseded by choosing the most common label in the test set. Statistically choosing the most common label should be better on average, but it fared worse when being used for examples that has no classifications. Of course the drawback of this method is that the statistically most common labels need to be known in advance. This method proved to be slightly better than by random on average.

Finally, we made a score system for trees when competing against each other. If there were multiple trees all with positive classifications then the tree that was chosen would receive a point if it correctly classified the result, or lose one if it provided a false positive. Then for the next classification the tree with the highest score would be chosen if there was contention. This improved the \( F_1 \) measure, but could not be used when no classes were found. Another drawback is that a different measure must be used the first time there is contention as there are no scores, and this can have a large impact on the path of evaluation. Also, it may not always be possible to get immediate feedback on the classification whilst learning, which would not make this possible. This method is also bound by the complexity of sorting the order of label priorities (n log n), so it may not be the best choice if there is a very large number of labels, and time is a factor. 

If we had more time, I would like to have trained a decision tree to choose which label to trust in the event of multiple positives, or even in other cases (for instance if it was always the case that if the two trees to return positive were Anger and Surprise then the true label was Happy, if all the other trees returned negative). It is worth noting that this would require a much larger sample of data.
